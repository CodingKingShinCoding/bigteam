{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 001 cost = 0.32405<br></br>\n",
    "Epoch: 011 cost = 0.10069<br></br>\n",
    "Epoch: 021 cost = 0.06607<br></br>\n",
    "Epoch: 031 cost = 0.04997<br></br>\n",
    "Epoch: 041 cost = 0.04423<br></br>\n",
    "Epoch: 051 cost = 0.04007<br></br>\n",
    "Epoch: 061 cost = 0.03610<br></br>\n",
    "Epoch: 071 cost = 0.03212<br></br>\n",
    "Epoch: 081 cost = 0.03288<br></br>\n",
    "Epoch: 091 cost = 0.02877<br></br>\n",
    "Epoch: 101 cost = 0.02845<br></br>\n",
    "Epoch: 111 cost = 0.02791<br></br>\n",
    "Epoch: 121 cost = 0.02519<br></br>\n",
    "Epoch: 131 cost = 0.02451<br></br>\n",
    "Epoch: 141 cost = 0.02363<br></br>\n",
    "Accuracy:  0.98382\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>training epoch 400->500 변경</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "def feature_normalize(features):\n",
    "    mu = np.mean(features,axis=0)\n",
    "    sigma = np.std(features,axis=0)\n",
    "    return (features - mu)/sigma\n",
    "\n",
    "def append_bias_reshape(features):\n",
    "    n_training_samples, n_dim  = features.shape[0], features.shape[1]\n",
    "    features = np.reshape(np.c_[np.ones(n_training_samples),features],\n",
    "    \t[n_training_samples,n_dim + 1])\n",
    "    return features\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data_set.csv',encoding='cp949')\n",
    "\n",
    "df.AGE = df.AGE.replace('*',0)\n",
    "df.AGE = pd.Series(df.AGE).convert_objects(convert_numeric=True)\n",
    "df.AGE = df.AGE.replace(0,df.AGE.mean())\n",
    "df.AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측 데이터가 많은 column 순으로 데이터 확인\n",
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.TEL_CNTT_QTR = df.TEL_CNTT_QTR.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.PAYM_METD = df.PAYM_METD.fillna('O')\n",
    "df.TEL_MBSP_GRAD = df.TEL_MBSP_GRAD.fillna('R')\n",
    "df.MATE_OCCP_NAME_G = df.MATE_OCCP_NAME_G.fillna('주부')\n",
    "df.OCCP_NAME_G = df.OCCP_NAME_G.fillna('주부')\n",
    "df.LAST_CHLD_AGE = df.LAST_CHLD_AGE.fillna(df.LAST_CHLD_AGE.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['CUST_ID']\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = df.TARGET\n",
    "train_x = df.drop('TARGET',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_features = feature_normalize(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(ratio=0.4,k=5,kind='regular',random_state=10)\n",
    "features_res, labels_res = sm.fit_sample(normalized_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(features_res)\n",
    "df_2 = pd.DataFrame(labels_res)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.columns = ['TARGET']\n",
    "df_final = pd.concat([df_1,df_2],axis=1)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = df_final.sample(frac=1)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_split = int(0.9*len(df_final))\n",
    "train_data = df_final#[:data_split]\n",
    "#test_data = df_final[data_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train_data.TARGET\n",
    "train_x = train_data.drop('TARGET',axis=1)\n",
    "#test_y = test_data.TARGET\n",
    "#test_x = test_data.drop('TARGET',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = one_hot_encode(train_y)\n",
    "#test_y = one_hot_encode(test_y)\n",
    "\n",
    "train_x = train_x.as_matrix()\n",
    "#test_x = test_x.as_matrix()\n",
    "\n",
    "print(len(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "batch_size = 200\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,207])\n",
    "Y = tf.placeholder(tf.float32,[None,2])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(\"W1\", shape=[207, 256],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.get_variable(\"W2\", shape=[256, 512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W3 = tf.get_variable(\"W5\", shape=[512, 2],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "b3 = tf.Variable(tf.random_normal([2]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(features_res) / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        randidx = np.random.randint(len(train_x),size=batch_size)\n",
    "        batch_xs = train_x[randidx,:]\n",
    "        batch_ys = train_y[randidx,:]\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys,keep_prob:0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    if epoch%10 ==0:\n",
    "        print('Epoch:', '%03d' % (epoch + 1), 'cost =', '{:.5f}'.format(avg_cost))\n",
    "        #print (\"Train_Accuracy: \",(sess.run(accuracy, feed_dict={X: train_x, Y: train_y,keep_prob:1})))\n",
    "        #print (\"Test_Accuracy: \",(sess.run(accuracy, feed_dict={X: test_x, Y: test_y,keep_prob:1})))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#print (\"Accuracy: \",(sess.run(accuracy, feed_dict={X: test_x, Y: test_y,keep_prob:1})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler \n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_sample(normalized_features, labels)\n",
    "\n",
    "df_1 = pd.DataFrame(features_res)\n",
    "df_2 = pd.DataFrame(labels_res)\n",
    "df_2.head()\n",
    "df_2.columns = ['TARGET']\n",
    "df_final = pd.concat([df_1,df_2],axis=1)\n",
    "df_final.head()\n",
    "df_final = df_final.sample(frac=1)\n",
    "df_final.head()\n",
    "final_y = df_final.TARGET\n",
    "final_x = df_final.drop('TARGET',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_sample(normalized_features, labels)\n",
    "\n",
    "df_1 = pd.DataFrame(features_res)\n",
    "df_2 = pd.DataFrame(labels_res)\n",
    "df_2.head()\n",
    "df_2.columns = ['TARGET']\n",
    "df_final = pd.concat([df_1,df_2],axis=1)\n",
    "df_final.head()\n",
    "df_final = df_final.sample(frac=1)\n",
    "df_final.head()\n",
    "final_y = df_final.TARGET\n",
    "final_x = df_final.drop('TARGET',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler ,normalize\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_ = train_data.TARGET\n",
    "train_x_ = train_data.drop('TARGET',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier(learning_rate=0.2,max_depth=7,base_score=0.5,max_delta_step=0,n_estimators=100)\n",
    "\n",
    "pipe = Pipeline([['xg',MinMaxScaler(feature_range=(0,5))],['model',model]])\n",
    "pipe.fit(train_x_,train_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"ACCURACY:\",pipe.score(final_x,final_y))\n",
    "print(classification_report(final_y,pipe.predict(final_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_y = one_hot_encode(final_y)\n",
    "tensor_x = final_x.as_matrix()\n",
    "print (\"TEST Accuracy: \",(sess.run(accuracy, feed_dict={X: tensor_x, Y: tensor_y,keep_prob:1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_p = tf.argmax(hypothesis, 1)\n",
    "test_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={X:tensor_x, Y:tensor_y,keep_prob:1})\n",
    "print (\"validation accuracy:\", val_accuracy)\n",
    "y_true = np.argmax(tensor_y,1)\n",
    "print (\"Precision\", sk.metrics.precision_score(y_true, y_pred))\n",
    "print (\"Recall\", sk.metrics.recall_score(y_true, y_pred))\n",
    "print (\"f1_score\", sk.metrics.f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
